# 端到端的机器学习项目

## 观察大局

1.总体目标:使用加州人口普查的数据建立起加州的房价模型。数据中有许多指标，诸如每个街区的人口数量、收入中位数、房价中位数等。你的模型需要从这个数据中学习，从而能够根据所有其他指标，预测任意区域的房价中位数。



数据流水线：一个序列的数据处理组件称为一个数据流水线。

> 组件通常是异步运行的。每个组件拉取大量的数据，然后进行处
理，再将结果传输给另一个数据仓库。一段时间之后，流水线中的下一
个组件会拉取前面的数据，并给出自己的输出，以此类推。每个组件都
很独立：组件和组件之间的连接只有数据仓库。这使得整个系统非常简
单易懂（在数据流图表的帮助下），不同团队可以专注于不同的组件。
如果某个组件发生故障，它的下游组件还能使用前面的最后一个输出继
续正常运行（至少一段时间），这使得系统比较健壮。



2.框架问题：

> 首先，你需要回答框
架问题：是有监督学习、无监督学习还是强化学习？是分类任务、回归
任务还是其他任务？应该使用批量学习还是在线学习技术？

显然，这是一个典型的有监督学习任
务，因为已经给出了标记的训练示例（每个实例都有预期的产出，也就


是该区域的房价中位数）。并且这也是一个典型的回归任务，因为你要
对某个值进行预测。更具体地说，这是一个多重回归问题，因为系统要
使用多个特征进行预测（使用区域的人口、收入中位数等）。这也是一
元回归问题，因为我们仅尝试预测每个区域的单个值。如果我们试图预
测每个区域的多个值，那将是多元回归问题。最后，我们没有一个连续
的数据流不断流进系统，所以不需要针对变化的数据做出特别调整，数
据量也不是很大，不需要多个内存，所以简单的批量学习应该就能胜
任。



3.选择性能指标：

回归问题的典型性能指标是均方根误差
（RMSE）。

![](D:\home\markdown-notebook\assets\WEBRESOURCEb6749619c0fb2e705ba9d5a2a38bc0b0.png)

4.检查假设：

最后，列举和验证到目前为止（由你或者其他人）做出的假设，是
一个非常好的习惯。

## 获取数据

1.下载数据

```python
import os         #os 模块提供了非常丰富的方法用来处理文件和目录。
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"   #下载的根目录地址
HOUSING_PATH = os.path.join("datasets", "housing")                               #本地存储目录地址
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"                     #下载地址

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)                                     #创建本地目录
    tgz_path = os.path.join(housing_path, "housing.tgz")                         #压缩包地址                    
    urllib.request.urlretrieve(housing_url, tgz_path)                            #从housing_url下载文件，保存到tgz_path
    housing_tgz = tarfile.open(tgz_path)                                         #打开tar归档文件
    housing_tgz.extractall(path=housing_path)                                    #解压文件
    housing_tgz.close()                                                          #关闭归档文件
```

```python
>>>fetch_housing_data()    #将数据从网络上下载解压，路径为HOUSING_PATH
```



```python
#使用pandas加载数据
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")        #csv文件路径
    return pd.read_csv(csv_path)                                #从csv_path中读取数据，返回一个DataFrame                                 
```

2.快速查看数据结构

![](D:\home\markdown-notebook\assets\WEBRESOURCE7dd6c6226b5c97dc865e245313485825.png)通过info（）方法可以快速获取数据集的简单描述，特别是总行
数、每个属性的类型和非空值的数量。

![](D:\home\markdown-notebook\assets\WEBRESOURCE94d681452b1d90d5b7bde8e149642366.png)> 通过对DataFrame对象调用info我们可以得到的信息有：总行
数、每个属性的类型和非空值的数量

> 其中total_bedrooms的非空值行共有20433行，但总行有20640行，共有207个空值

通过观察前5行数据，只有ocean_proximity列的值不是数字。他们是字符串，且是重复的，所以该列可能是个分类属性。

```python
"""
housing["列名"]：获取DataFrame对象的一列
Series.values   用于计算一个Series中各值出现的频率：
"""
>>>housing['ocean_proximity'].value_counts()
<1H OCEAN     9136
INLAND        6551
NEAR OCEAN    2658
NEAR BAY      2290
ISLAND           5
Name: ocean_proximity, dtype: int64

```

> 使用value_counts（）方法查看有多少种分类存在，每种类别下分
别有多少个区域

再来看看其他属性，通过describe（）方法可以显示数值属性的摘
要:



![](D:\home\markdown-notebook\assets\WEBRESOURCE0d173cad2a79f5ee90611977e2eba425.png)> std:标准差，用于衡量数据的离散程度

> 25%,50%,75%:百分位数，表示给定百分比的值给低于当前值

另一种快速了解数据类型的方法是绘制每个数值属性的直方图。直
方图用来显示给定值范围（横轴）的实例数量（纵轴）。

```python
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50,figsize=(20,15))    #将housing的每一列绘制成直方图
plt.show()
```



![](D:\home\markdown-notebook\assets\WEBRESOURCE2c6749698ff4516a8da7e746f606a334.png)![](D:\home\markdown-notebook\assets\WEBRESOURCE6b8a5f3dd3d172ae9bcb4571e738f30a.png)

![](D:\home\markdown-notebook\assets\WEBRESOURCE877f2f8c09975ff51967a50469c1a30b.png)

> 1.首先，收入中位数这个属性看起来不像是用美元（USD）在衡量。经与收集数据的团队核实，你得知数据已经按比例缩小，并框出中位数的上限为15（实际为15.0001），下限为0.5（实际为0.4999）。数字后的单位为万美元，例如，15代表15万美元。在机器学习中，使用经过预处理的属性是很常见的事情，倒不一定是个问题，但是你至少需要了解数据是如何计算的。

> 2.房龄中位数和房价中位数也被设定了上限。而由于后者正是你的目标属性（标签），因此这可能是个大问题。你的机器学习算法很可能会学习到价格永远不会超过这个限制。

> 3.这些属性值被缩放的程度各不相同

> 4.最后，许多直方图都表现出重尾：图形在中位数右侧的延伸比左侧要远得多。这可能会导致某些机器学习算法难以检测模式。

3.创建测试集

理论上，创建测试集非常简单，只需要随机选择一些实例，通常是数据集的20%（如果数据集很大，比例将更小），然后将它们放在一边：

```python
#创建测试集(有缺陷)
import numpy as np
def split_train_test(data,test_ratio):
    shuffled_indices=np.random.permutation(len(data))  #返回和data一样大的，从0到len(data)范围随机排序的ndarray
    test_set_size=int(len(data)*test_ratio)            #计算测试集的大小          
    test_indices=shuffled_indices[:test_set_size]      #测试的索引范围
    train_indices=shuffled_indices[test_set_size:]     #训练的索引范围
    return data.iloc[train_indices],data.iloc[test_indices]    #返回训练数据集和测试集
```

使用方法：

![](D:\home\markdown-notebook\assets\WEBRESOURCE0a6b9a2b745555a50d6ca5c172076a69.png)> 是的，这确实能行，但这并不完美。如果再运行一遍，它又会产生
一个不同的数据集！这样下去，你（或者你的机器学习算法）将会看到
整个完整的数据集，而这正是创建测试集时需要避免的。

你可以计算每个实例标识符的哈希值，如果这个哈希值小于或等于最大哈希值的20%，则将该实例放入
测试集。这样可以确保测试集在多个运行里都是一致的，即便更新数据
集也仍然一致。新实例的20%将被放入新的测试集，而之前训练集中的
实例也不会被放入新测试集。

```python
from zlib import crc32

def test_set_check(identifier,test_ratio):        #给定测试集占比，根据标识符判断是否应该被纳入测试集
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio*2**32    

def split_train_test_by_id(data,test_ratio,id_column):
    ids=data[id_column]                                               #ids是data中作为标识符的列
    in_test_set=ids.apply(lambda id_:test_set_check(id_,test_ratio))  #对ids中的每个标识符执行检查，返回一个布尔数组
    return data.loc[~in_test_set],data.loc[in_test_set]               #根据布尔数组获取测试集和训练集             
```

使
用行索引作为ID：



```python
housing_with_id=housing.reset_index()                                   #增加了index一列
train_set,test_set=split_train_test_by_id(housing_with_id,0.2,"index")
```

如果使用行索引作为唯一标识符，你需要确保在数据集的末尾添加新数据，并且不会删除任何行。如果不能保证这一点，那么你可以尝试使用某个最稳定的特征来创建唯一标识符。例如，一个区域的经纬度肯定几百万年都不会变，你可以将它们组合成如下的ID。

```python
housing_with_id['id']=housing["longitude"]*1000+housing["latitude"]
train_set,test_set=split_train_test_by_id(housing_with_id,0.2,"id")
```

Scikit-Learn提供了一些函数，可以通过多种方式将数据集分成多
个子集，最简单的函数是train_test_split（）。

> 前面使用的都是纯随机的抽样方法。如果数据集足够庞大（特别是相较于属性的数量而言），这种方式通常不错；如果不是，则有可能会导致明显的抽样偏差。

> 分层抽样:分层抽样概述

> 

收入中位数是预测房价的重要属性，于是你希望确保在收入属性上，测试集能够代表整个数据集中各种不同类型的收入。由于收入中位数是一个连续的数值属性，所以你得先创建一个收入类别的属性。我们先来看一下收入中位数的直方图：大多数收入中位数值聚集在1.5～6（15000～60 000美元）左右，但也有一部分远远超过了6万美元。

![](D:\home\markdown-notebook\assets\WEBRESOURCEc326c241bb18f347ed7c437c8640defc.png)在数据集中，每一层都要有足够数量的实例，这一点至关重要，不然数据不足的层，其重要程度很有可能会被错估。也就是说，你不应该将层数分得太多，每一层应该要足够大才行。下面这段代码是用pd.cut（）来创建5个收入类别属性的（用1～5来做标签），0～1.5是类别1，1.5～3是类别2，以此类推：

```python
housing["income_cat"]=pd.cut(housing["median_income"],bins=[0.,1.5,3.0,4.5,6.,np.inf],labels=[1,2,3,4,5])
"""
bins指定了划分的范围边界，即将housing["median_income"]的列划分为(0,1.5],(1.5,3],(3,4.5],(4.5,5],(5,inf)这5个范围
并为他们指定了标签1至5
将新产生的标签列加入到housing中。相当与对该列执行分类任务
"""
```

查看分类情况:

![](D:\home\markdown-notebook\assets\WEBRESOURCEc33990047ba022bb9ee3a4a881421ca9.png)使用Scikit-Learn的
StratifiedShuffleSplit类进行分类抽样:

```python
from sklearn.model_selection import StratifiedShuffleSplit
split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)            #分层 ShuffleSplit 交叉验证器
for train_index,test_index in split.split(housing,housing["income_cat"]):        #split.split(x,y)生成索引,以将数据拆分为训练集和测试集,x是数据，y是标签
    strat_train_set=housing.loc[train_index]
    strat_test_set=housing.loc[test_index]
```

查看测试集中收入类别比例分布：

```python
>>>strat_test_set["income_cat"].value_counts()/len(strat_test_set)
3    0.350533
2    0.318798
4    0.176357
5    0.114341
1    0.039971
Name: income_cat, dtype: float64
```

现在你可以删除income_cat属性，将数据恢复原样了：

```python
for set_ in (strat_train_set,strat_test_set):
    set_.drop("income_cat",axis=1,inplace=True)    #axis=1指定列
```

## 从数据探索和可视化中获得洞见

```python
housing = strat_train_set.copy()        #先创建一个数据副本
```

1.将地理数据可视化

由于存在地理位置信息（经度和纬度），因此建立一个各区域的分布图以便于可视化数据是一个很好的想法。

```python
#从housing中选出longitude作为x轴，latitude作为y轴绘制散点图
housing.plot(kind="scatter",x="longitude",y="latitude")
```

![](D:\home\markdown-notebook\assets\WEBRESOURCE62ac1eb177c14b9df609c966ccf89dbc.png)```python
#突出高密度区域
housing.plot(kind='scatter',x="longitude",y="latitude",alpha=0.1)
```

将目标平均房价加入到散点图中：

```python
"""
kind="scatter":散点图
x="longitude",y="latitude"                :指定图像的x轴和y轴
alpha=0.4                                 :透明度设置,用于突出密集区域
s=housing["population"]/100               :点的大小
label="population"                        :点的标签
figsize=(10,7)                            :图像大小
c="median_house_value"                    :每个点的颜色由表的median_house_value列决定
cmap=plt.get_cmap("jet")                  :get_cmp("jet")‎返回‎‎"jet"的色彩映射表‎‎实例，赋值给cmap‎。
colorbar=True                             :显示颜色条
"""
housing.plot(kind="scatter",x="longitude",y="latitude",alpha=0.4,
            s=housing["population"]/100,label="population",figsize=(10,7),
            c="median_house_value",cmap=plt.get_cmap("jet"),colorbar=True)
plt.legend()                         #在轴上放置一个图例，图中population旁的圆点
```



![](D:\home\markdown-notebook\assets\WEBRESOURCE640a9b97f513598fd835b26987a03bc5.png)> 此图相当于一个四维图，可以看出经度，纬度，人口和房间的关系。加州房价：红色昂贵，蓝色便宜，较大的圆圈表示人口较多的地区。

2.寻找相关性

由于数据集不大，你可以使用corr（）方法轻松计算出每对属性之间的标准相关系数（也称为皮尔逊r）：??

```python
>>>corr_matrix=housing.corr()            #包含所有列相关系数数据的矩阵
"""
corr_matrix["median_house_value"]     #获取median_house_value列，该列表示median_house_value与其他所有属性的相关系数

"""
>>>corr_matrix["median_house_value"].sort_values(ascending=False)    #相关系数按值排序，降序
median_house_value    1.000000
median_income         0.687151
total_rooms           0.135140
housing_median_age    0.114146
households            0.064590
total_bedrooms        0.047781
population           -0.026882
longitude            -0.047466
latitude             -0.142673
Name: median_house_value, dtype: float64
```

> 相关系数的范围从-1变化到1。越接近1，表示有越强的正相关。例如，当收入中位数上升时，房价中位数也趋于上升。当系数接近于-1时，表示有较强的负相关。

我们可以看到纬度和房价中位数之间呈现出轻微的负相关（也就是说，越往北走，房价倾向于下降）。最后，系数靠近0则说明二者之间没有线性相关性。下图显示了横轴和纵轴之间相关性系数的多种图像:

![](D:\home\markdown-notebook\assets\WEBRESOURCE5e8a18be4d3c9c8ec4428a4f904a7ce9.png)相关系数仅测量线性相关性（“如果x上升，则y上升/下降”）。所以它有可能彻底遗漏非线性相关性（例如“如果x接近于0，则y会上升”）。注意图最下面一排的图像，它们的相关性系数都是0，但是显然我们可以看出横轴和纵轴之间的关系并不是彼此完全独立的：这是非线性关系的示例。需要注意的是相关性跟斜率完全无关。

还有一种方法可以检测属性之间的相关性，就是使用pandas的scatter_matrix函数，它会绘制出每个数值属性相对于其他数值属性的相关性：

```python
from pandas.plotting import scatter_matrix
attributes=["median_house_value","median_income","total_rooms",
           "housing_median_age"]
scatter_matrix(housing[attributes],figsize=(12,8))    #画出矩阵散点图
"""
这张图分为两部分：对角线部分和非对角线部分。
1.对角线部分： 核密度估计图（Kernel Density Estimation），就是用来看某 一个 变量分布情况，横轴对应着该变量的值，纵轴对应着该变量的密度（可以理解为出现频次）。
2.非对角线部分：两个 变量之间分布的关联散点图。将任意两个变量进行配对，以其中一个为横坐标，另一个为纵坐标，将所有的数据点绘制在图上，用来衡量两个变量的关联度（Correlation）。
"""
```

![](D:\home\markdown-notebook\assets\WEBRESOURCE53cf7dec7548ec0849e7f99a2fc6f026.png)最有潜力能够预测房价中位数的属性是收入中位数，所以我们放大来看看其相关性的散点图。

```python
housing.plot(kind="scatter", x="median_income", y="median_house_value",alpha=0.1)
```

![](D:\home\markdown-notebook\assets\WEBRESOURCE979ff13208fb346b1a12586954b2acb1.png)> 该图说明了几个问题。首先，二者的相关性确实很强，你可以清楚地看到上升的趋势，并且点也不是太分散。其次，前面我们提到过50万美元的价格上限在图中是一条清晰的水平线，不过除此之外，图还显示出几条不那么明显的直线：45万美元附近有一条水平线，35万美元附近也有一条，28万美元附近似乎隐约也有一条，再往下可能还有一些。为了避免你的算法学习之后重现这些怪异数据，你可能会尝试删除这些相应区域。

3.试验不同属性的组合

在准备给机器学习算法输入数据之前，你要做的最后一件事应该是尝试各种属性的组合。例如，如果你不知道一个区域有多少个家庭，那么知道一个区域的“房间总数”也没什么用。你真正想要知道的是一个家庭的房间数量。同样，单看“卧室总数”这个属性本身也没什么意义，你可能想拿它和“房间总数”来对比，或者拿来同“每个家庭的人口数”这个属性组合似乎也挺有意思。我们来试着创建这些新属性：

```python
#组合一些属性
housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"]=housing["population"]/housing["households"]
#计算相关性
corr_matrix=housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
median_house_value          1.000000
median_income               0.687151
rooms_per_household         0.146255
total_rooms                 0.135140
housing_median_age          0.114146
households                  0.064590
total_bedrooms              0.047781
population_per_household   -0.021991
population                 -0.026882
longitude                  -0.047466
latitude                   -0.142673
bedrooms_per_room          -0.259952
Name: median_house_value, dtype: float64
```

> 新属性bedrooms_per_room较之“房间总数”或“卧室总数”与房价中位数的相关性都要高得多。

## 机器学习算法的数据准备

让我们先回到一个干净的训练集（再次复制strat_train_set），然后将预测器和标签分开，因为这里我们不一定对它们使用相同的转换方式（需要注意drop（）会创建一个数据副本，但是不影响strat_train_set）：

```python
housing=strat_train_set.drop("median_house_value",axis=1)            #housing是算法输入属性
housing_labels=strat_train_set["median_house_value"].copy()          #housing_labels是标签
```

1.数据清理

大部分的机器学习算法无法在缺失的特征上工作，所以我们要创建一些函数来辅助它。前面我们已经注意到total_bedrooms属性有部分值缺失，所以我们要解决它。有以下三种选择：

- 放弃这些相应的区域。


- 放弃整个属性。


- 将缺失的值设置为某个值（0、平均数或者中位数等）。

```python
"""
去除表中的缺失值:
    若表中的total_bedrooms列含有缺失值，删除表中对应的行
"""
housing.dropna(subset=["total_bedrooms"])                                        #方法1
housing.drop("total_bedrooms",axis=1)            #删除表中total_bedrooms列         方法2
#方法3
median=housing["total_bedrooms"].median()        #计算表中total_bedrooms列的均值        
housing["total_bedrooms"].fillna(median,inplace=True)    #以均值填充表中total_bedrooms中的缺失值
```

Scikit-Learn
提供了一个非常容易上手的类来处理缺失值：SimpleImputer。使用方
法如下：首先，你需要创建一个SimpleImputer实例，指定你要用属性
的中位数值替换该属性的缺失值：

```python
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy="median")
#由于中位数值只能在数值属性上计算，所以我们需要创建一个没有文本属性ocean_proximity的数据副本：
housing_num=housing.drop("ocean_proximity",axis=1)   #housing_num是去除文本数据的表

#这里imputer仅仅只是计算了每个属性的中位数值，并将结果存储在其实例变量statistics_中。
#虽然只有total_bedrooms这个属性存在缺失值，但是我们无法确认系统启动之后新数据中是否一定不存在任何缺失值，
#所以稳妥起见，还是将imputer应用于所有的数值属性：
imputer.fit(housing_num)                             #使用fit（）方法将imputer实例适配到训练数据：
imputer.statistics_
array([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,
       1164.     ,  408.     ,    3.54155])
housing_num.median().values
array([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,
       1164.     ,  408.     ,    3.54155])
```

现在，你可以使用这个“训练有素”的imputer将缺失值替换成中位数值从而完成训练集转换：

```python
X=imputer.transform(housing_num)    #transform方法使用imputer实例计算的均值替换housing_num中的缺失值,返回一个numpy数组
#再次将numpy数组转化成DataFrame
housing_tr=pd.DataFrame(X,columns=housing_num.columns,index=housing_num.index)
```

---

Scikit-Learn的设计

1.估算器

能够根据数据集对某些参数进行估算的任意对象都可以称为估算器（例如，imputer就是一个估算器）。估算由fit（）方法执行，它只需要一个数据集作为参数（或者两个——对于有监督学习算法，第二个数据集包含标签）。引导估算过程的任何其他参数都视为超参数（例如，imputer’s strategy），它必须被设置为一个实例变量（一般通过构造函数参数）。



2.转换器

有些估算器（例如imputer）也可以转换数据集，这些称为转换器。同样，API也非常简单：由transform（）方法和作为参数的待转换数据集一起执行转换，返回的结果就是转换后的数据集。这种转换的过程通常依赖于学习的参数，比如本例中的imputer。所有的转换器都可以使用一个很方便的方法，即fit_transform（），相当于先调用fit（）然后再调用transform（）（但是fit_transform（）有时是被优化过的，所以运行得更快一些）。



3.预测器

最后，还有些估算器能够基于一个给定的数据集进行预测，这称为预测器。例如LinearRegression模型就是一个预测器，它基于一个国家的人均GDP预测该国家的生活满意度。预测器的predict（）方法会接受一个新实例的数据集，然后返回一个包含相应预测的数据集。值得一提的还有一个score（）方法，可以用来衡量给定测试集的预测质量（以及在有监督学习算法里对应的标签）。



检查：

所有估算器的超参数都可以通过公共实例变量（例如，imputer.strategy）直接访问，并且所有估算器的学习参数也可以通过有下划线后缀的公共实例变量来访问（例如，imputer.statistics）。



防止类扩散

数据集被表示为NumPy数组或SciPy稀疏矩阵，而不是自定义的类型。超参数只是普通的Python字符串或者数字。

---

2.处理文本和分类属性

在此数据集中，文本属性只有一个：ocean_proximity属性。我们看看前10个实例的值：

```python
housing_cat=housing[["ocean_proximity"]]
housing_cat.head(10)
        ocean_proximity
12655	INLAND
15502	NEAR OCEAN
2908	INLAND
14053	NEAR OCEAN
20496	<1H OCEAN
1481	NEAR BAY
18125	<1H OCEAN
5830	<1H OCEAN
17989	<1H OCEAN
4861	<1H OCEAN
```

它不是任意文本，而是有限个可能的取值，每个值代表一个类别。因此，此属性是分类属性。大多数机器学习算法更喜欢使用数字，因此让我们将这些类别从文本转到数字。为此，我们可以使用Scikit-Learn的OrdinalEncoder类。

```python
from sklearn.preprocessing import OrdinalEncoder
"""
OrdinalEncoder:
将分类特征编码为整数数组。这个转换器的输入应该是一个类似数组的整数或字符串，表示分类（离散）特征所采用的值。
 特征被转换为序数整数。这会导致每个特征有一列整数（0 到 n_categories - 1）。
"""
ordinal_encoder=OrdinalEncoder()
housing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)    #对housing_cat列进行分类，每个类给一个编号。返回一个由编号组成的numpy数组。
housing_cat_encoded[:10]                                          #查看编号数组的前10个
array([[1.],
       [4.],
       [1.],
       [4.],
       [0.],
       [3.],
       [0.],
       [0.],
       [0.],
       [0.]])
```

你可以使用Categories_实例变量获取类别列表。这个列表包含每个类别属性的一维数组（在这种情况下，这个列表包含一个数组，因为只有一个类别属性）：

```python
ordinal_encoder.categories_
[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
       dtype=object)]
```

这种表征方式产生的一个问题是，机器学习算法会认为两个相近的值比两个离得较远的值更为相似一些。在某些情况下这是对的（对一些有序类别，像“坏”“平均”“好”“优秀”），但是，对ocean_proximity而言情况并非如此（例如，类别0和类别4之间就比类别0和类别1之间的相似度更高）。为了解决这个问题，常见的解决方案是给每个类别创建一个二进制的属性：当类别是“<1H OCEAN”时，一个属性为1（其他为0），当类别是“INLAND”时，另一个属性为1（其他为0），以此类推。这就是独热编码，因为只有一个属性为1（热），其他均为0（冷）。新的属性有时候称为哑（dummy）属性。Scikit-Learn提供了一个OneHotEncoder编码器，可以将整数类别值转换为独热向量。

```python
from sklearn.preprocessing import OneHotEncoder
cat_encoder=OneHotEncoder()
housing_cat_1hot=cat_encoder.fit_transform(housing_cat)            #将housing_cat列转换成独热编码
housing_cat_1hot                                                   
<16512x5 sparse matrix of type '<class 'numpy.float64'>'
	with 16512 stored elements in Compressed Sparse Row format>
 """
注意到这里的输出是一个SciPy稀疏矩阵，在属性很多的时候很有用。
使用toarray可以将其转换成普通的二维numpy数组
 """
 housing_cat_1hot.toarray()
 array([[0., 1., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0.],
       ...,
       [1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.]])
```

你可以再次使用编码器的categories_实例变量来得到类别列表：

```python
cat_encoder.categories_
[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
       dtype=object)]
```



3.自定义转换器

虽然Scikit-Learn提供了许多有用的转换器，但是你仍然需要为一些诸如自定义清理操作或组合特定属性等任务编写自己的转换器。你当然希望让自己的转换器与Scikit-Learn自身的功能（比如流水线）无缝衔接，而由于Scikit-Learn依赖于鸭子类型的编译，而不是继承，所以你所需要的只是创建一个类，然后应用以下三种方法：fit（）（返回self）、transform（）、fit_transform（）。

```python
from sklearn.base import BaseEstimator,TransformerMixin

rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6
"""
自定义转换器:
    1.继承BaseEstimator,和TransformerMixin
    2.__init__方法可用于添加控制
    3.fit方法必须返回self
    4.transform方法用于处理数据
"""
class CombinedAttributesAdder(BaseEstimator,TransformerMixin):
    def __init__(self,add_bedrooms_per_room=True):        #no *args or **kargs
        self.add_bedrooms_per_room=add_bedrooms_per_room
    def fit(self,X,y=None):
        return self    #nothing else to do
    def transform(self,X):                                                        #自定义转换器中的数据是numpy数组，此时X是二维numpy数组
        rooms_per_household=X[:,rooms_ix]/X[:,households_ix]                      #将X中的第5列（即population列）中每个元素与X中的第4列（即households列）相除，组成rooms_per_household列
        population_per_household=X[:,population_ix]/X[:,rooms_ix]                 #同理生成population_per_household列
        if self.add_bedrooms_per_room:
            bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]
            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]    #np.c_[]函数将数组沿水平方向，从左到右组成一个新的二维数组
        else:
            return np.c_[X,rooms_per_household,population_per_household]
    
attr_adder=CombinedAttributesAdder(add_bedrooms_per_room=False)    
housing_extra_attribs=attr_adder.transform(housing.values)                                    #只能向转换器中传递numpy数组
```

在本例中，转换器有一个超参数add_bedrooms_per_room默认设置为True（提供合理的默认值通常是很有帮助的）。这个超参数可以让你轻松知晓添加这个属性是否有助于机器学习算法。更一般地，如果你对数据准备的步骤没有充分的信心，就可以添加这个超参数来进行把关。这些数据准备步骤的执行越自动化，你自动尝试的组合也就越多，从而有更大可能从中找到一个重要的组合（还节省了大量时间）。



4.特征缩放

最重要也最需要应用到数据上的转换就是特征缩放。如果输入的
数值属性具有非常大的比例差异，往往会导致机器学习算法的性能表现不佳，当然也有极少数特例。案例中的房屋数据就是这样：房间总
数的范围从6～39 320，而收入中位数的范围是0～15。注意，目标值
通常不需要缩放。

同比例缩放所有属性的两种常用方法是最小-最大缩放和标准化:

最小-最大缩放（又叫作归一化）很简单：将值重新缩放使其最终
范围归于0～1之间。实现方法是将值减去最小值并除以最大值和最小
值的差。对此，Scikit-Learn提供了一个名为MinMaxScaler的转换
器。如果出于某种原因，你希望范围不是0～1，那么可以通过调整超
参数feature_range进行更改。

标准化则完全不一样：首先减去平均值（所以标准化值的均值总是零），然后除以方差，从而使得结果的分布具备单位方差。不同于最小-最大缩放的是，标准化不将值绑定到特定范围，对某些算法而言，这可能是个问题（例如，神经网络期望的输入值范围通常是0～1）。但是标准化的方法受异常值的影响更小。例如，假设某个地区的平均收入为100（错误数据），最小-最大缩放会将所有其他值从0～15降到0～0.15，而标准化则不会受到很大影响。Scikit-Learn提供了一个标准化的转换器StandadScaler。



5.转换流水线

许多数据转换的步骤需要以正确的顺序来执行。而Scikit-Learn正好提供了Pipeline类来支持这样的转换。下面是一个数值属性的流水线示例：

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
 """
 Pipeline构造函数会通过一系列名称/估算器的配对来定义步骤序
列。
 除了最后一个是估算器之外，前面都必须是转换器（也就是说，
必须有fit_transform（）方法）。
 至于命名可以随意，你喜欢就好
（只要它们是独一无二的，不含双下划线），它们稍后在超参数调整
中会有用。
 
当调用流水线的fit（）方法时，会在所有转换器上按照顺序依次
调用fit_transform（），
将一个调用的输出作为参数传递给下一个调
用方法，直到传递到最终的估算器，则只会调用fit（）方法。

流水线的方法与最终的估算器的方法相同。
在本例中，最后一个估算器是StandardScaler，这是一个转换器，因此流水线有一个transform（）方法，
可以按顺序将所有的转换应用到数据中（这也是我们用过的fit_transform（）方法）。
"""

num_pipeline=Pipeline([
    ('imputer',SimpleImputer(strategy="median")),
    ('attribs_adder',CombinedAttributesAdder()),
    ('std_scaler',StandardScaler())
])
housing_num_tr=num_pipeline.fit_transform(housing_num)
```

到目前为止，我们分别处理了类别列和数值列。拥有一个能够处
理所有列的转换器会更方便，将适当的转换应用于每个列。在0.20版
中，Scikit-Learn为此引入了ColumnTransformer，好消息是它与
pandas DataFrames一起使用时效果很好。让我们用它来将所有转换应
用到房屋数据：

```python
"""
首先导入ColumnTransformer类，接下来获得数值列名称列表和类
别列名称列表，然后构造一个ColumnTransformer。
构造函数需要一个
元组列表，其中每个元组都包含一个名字、一个转换器，以及一个
该转换器能够应用的列名字（或索引）的列表。
在此示例中，我们指
定数值列使用之前定义的num_pipeline进行转换，类别列使用
OneHotEncoder进行转换。
最后，我们将ColumnTransformer应用到房
屋数据：它将每个转换器应用于适当的列，并沿第二个轴合并输出
（转换器必须返回相同数量的行）。
"""
from sklearn.compose import ColumnTransformer
num_attribs=list(housing_num)
cat_attribs=["ocean_proximity"]

full_pipeline=ColumnTransformer([
    ("num",num_pipeline,num_attribs),
    ("cat",cat_encoder,cat_attribs)

])
housing_prepared=full_pipeline.fit_transform(housing)
```

> 请注意，OneHotEncoder返回一个稀疏矩阵，而num_pipeline返回
一个密集矩阵。当稀疏矩阵和密集矩阵混合在一起时，
ColumnTransformer会估算最终矩阵的密度（即单元格的非零比率），
如果密度低于给定的阈值，则返回一个稀疏矩阵（通过默认值为
sparse_threshold=0.3）。在此示例中，它返回一个密集矩阵。

我们
得到了一个预处理流水线，该流水线可以获取全部房屋数据并对每一列进
行适当的转换。

## 选择和训练模型

1.训练和评估训练集

先训练一个线性模型：

```python
from sklearn.linear_model import LinearRegression

lin_reg=LinearRegression()
lin_reg.fit(housing_prepared,housing_labels)
```

现在你有一个可以工作的线性回归模型了。让我们用几个训练集的实例试试：

```python
some_data=housing.iloc[:5]
some_labels=housing_labels.iloc[:5]
some_data_prepared=full_pipeline.transform(some_data)
print("Predictions:",lin_reg.predict(some_data_prepared))
print("labels:",list(some_labels))
Predictions: [ 85749.86374743 305433.2153269  152019.84439021 185977.83516103
 244556.65849285]
labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]
```

我们可以使用Scikit-Learn的mean_squared_error（）
函数来测量整个训练集上回归模型的RMSE(以均方根误差为性能指标)：

```python
from sklearn.metrics import mean_squared_error
housing_prections=lin_reg.predict(housing_prepared)
lin_mse=mean_squared_error(housing_labels,housing_prections)
lin_rmse=np.sqrt(lin_mse)
lin_rmse
68634.00056099873
```

> 大多数区域的median_housing_values分布在120 000～265 000美
元之间，所以典型的预测误差达到68 634美元只能算是差强人意。这
就是一个典型的模型对训练数据欠拟合的案例。这种情况发生时，通
常意味着这些特征可能无法提供足够的信息来做出更好的预测，或者
是模型本身不够强大。想要修正欠拟合，可以
通过选择更强大的模型，或为算法训练提供更好的特征，又或者减少
对模型的限制等方法。

训练一个DecisionTreeRegressor:

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg=DecisionTreeRegressor()
tree_reg.fit(housing_prepared,housing_labels)
housing_predictions=tree_reg.predict(housing_prepared)
tree_mse=mean_squared_error(housing_labels,housing_prections)
tree_rmse=np.sqrt(tree_mse)
tree_rmse
0.0
#该模型可能对数据严重过拟合
```

2.使用交叉验证来更好地进行评估

评估决策树模型的一种方法是使用train_test_split函数将训练
集分为较小的训练集和验证集，然后根据这些较小的训练集来训练模
型，并对其进行评估。

另一个不错的选择是使用Scikit-Learn的K-折交叉验证功能。以
下是执行K-折交叉验证的代码：它将训练集随机分割成10个不同的子
集，每个子集称为一个折叠，然后对决策树模型进行10次训练和评估
——每次挑选1个折叠进行评估，使用另外的9个折叠进行训练。产生
的结果是一个包含10次评估分数的数组：

```python
from sklearn.model_selection import cross_val_score
scores=cross_val_score(tree_reg,housing_prepared,housing_labels,
    scoring="neg_mean_squared_error",cv=10)
tree_rmse_scores=np.sqrt(-scores)
"""
Scikit-Learn的交叉验证功能更倾向于使用效用函数（越大
越好）而不是成本函数（越小越好），所以计算分数的函数实际上是
负的MSE（一个负值）函数，这就是为什么上面的代码在计算平方根之
前会先计算出-scores
"""
```

查看结果:

```python
def display_scores(scores):
    print("Scores:",scores)
    print("Mean:",scores.mean())
    print("Standard deveation:",scores.std())

display_scores(tree_rmse_scores)
Scores: [69760.91982036 69248.25506001 67098.68594746 73378.21134686
 69974.52793741 73324.3192717  72304.8430908  73208.02128213
 68784.53710886 71446.97688434]
Mean: 70852.9297749936
Standard deveation: 2083.3594198181777
"""
请注意，交叉验证不仅可以得到一个模
型性能的评估值，还可以衡量该评估的精确度（即其标准差）。这里
该决策树得出的评分约为70852，上下浮动±2083。如果你只使用了
一个验证集，就收不到这样的结果信息。交叉验证的代价就是要多次
训练模型，因此也不是永远都行得通。
"""
```

计算一下线性回归模型的评分：

```python
lin_scores=cross_val_score(lin_reg,housing_prepared,housing_labels,
    scoring="neg_mean_squared_error",cv=10)
lin_rmse_scores=np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)
Scores: [71747.47345836 64119.2533148  67651.45880458 68664.27200399
 67420.92576712 72534.71844609 74019.63215464 68820.88568643
 66445.90842704 70143.35199184]
Mean: 69156.78800548989
Standard deveation: 2847.856858016588
"""
决策树模型确实是严重过拟合了:使用训练集数据，它的RMSE为0。但使用验证集数据，它的RMSE很大。
通过使用交叉验证对决策树，和线性回归模型进行评估，发现决策树模型还不如线性回归模型。
"""
```

训练一个RandomForestRegressor:

```python
from sklearn.ensemble import RandomForestRegressor
forest_reg=RandomForestRegressor()
forest_reg.fit(housing_prepared,housing_labels)
#训练集上的RMSE
housing_prections=forest_reg.predict(housing_prepared)
forest_mse=mean_squared_error(housing_labels,housing_prections)
forest_rmse=np.sqrt(forest_mse)
forest_rmse
18166.318336916618
#验证集上的RMSE
forest_scores=cross_val_score(forest_reg,housing_prepared,housing_labels,
    scoring="neg_mean_squared_error",cv=5)
forest_rmse_scores=np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)
Scores: [49109.74939411 48837.78324588 48956.49409315 49735.0393919
 50613.89580356]
Mean: 49450.59238571992
Standard deveation: 658.7908698198694
"""
随机森林看起来比较好。但是，请注意，训
练集上的分数仍然远低于验证集，这意味着该模型仍然对训练集过拟
合。
过拟合的可能解决方案包括简化模型、约束模型（即使其正规
化），或获得更多的训练数据。
"""
```

每一个尝试过的模型你都应该妥善保存，以便将来可以轻松
回到你想要的模型当中。记得还要同时保存超参数和训练过的参数，
以及交叉验证的评分和实际预测的结果。这样你就可以轻松地对比不
同模型类型的评分，以及不同模型造成的错误类型。通过Python的
pickle模块或joblib库，你可以轻松保存Scikit-Learn模型。



## 微调模型

假设你现在有了一个有效模型的候选列表。现在你需要对它们进
行微调。我们来看几个可行的方法。

1.网格搜索

