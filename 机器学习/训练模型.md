# 线性回归

线性模型就是对输入特征**加权求和**，再加上一个我 们称为**偏置项**（也称为截距项）的常数，以此进行预测。
$$
\hat{y}=\theta _{0}  +\theta _{1}\chi _{1} +\theta _{2}\chi _{2}+\cdots +\theta _{n}\chi _{n}
$$

- $$
  \hat{y}是预测值
  $$

- $$
  n是特征数量
  $$

- $$
  \chi _{i}是第i个特征值
  $$

- $$
\theta_{j}是第j个模型参数
$$

$$
\hat{y}=\theta _{0}  +\theta _{1}\chi _{1} +\theta _{2}\chi _{2}+\cdots +\theta _{n}\chi _{n}
$$

$$
=\theta _{0}\chi _{0}  +\theta _{1}\chi _{1} +\theta _{2}\chi _{2}+\cdots +\theta _{n}\chi _{n}
$$

$$
=\left ( \theta _{0},\theta _{1}\dots ,\theta _{n} \right ) 
\begin{pmatrix}1
 \\\chi_{1}
 \\: 
 \\\chi_{n}

\end{pmatrix}
$$

$$
设\theta=\left ( \theta _{0},\theta _{1}\dots ,\theta _{n} \right ) ^{\top } ,x=\begin{pmatrix}1
 \\\chi_{1}
 \\: 
 \\\chi_{n}

\end{pmatrix}	其中\theta与x均为列向量
$$

**故预测函数为：**
$$
\hat{y}=\theta ^{\top }x 
$$
<u>训练模型就是设置模型参数直到模型最拟合训练集的过程。</u>

回归模型最常见的性能指标是**均方根误差 （RMSE）**。因此，在训练线性回归模型时，你需要找到 最小化RMSE的θ值。在实践中，将**均方误差（MSE）**最小化比最小化RMSE更为简单，二者效果相同（因为使函数最小化的值，同样也使其平 方根最小)。
$$
RMSE(X,h)=\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left ( h\left ( x^{i} \right ) -y^{i} \right )^2   }
$$

$$
MSE(X,h_{\theta })=\frac{1}{m}\sum_{i=1}^{m}\left ( h\left ( x^{i} \right ) -y^{i} \right )^2    
$$

## 标准方程

为了得到使成本函数最小的θ值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即标准方程：
$$
\hat{\theta }=\left ( X^{\top }X \right )^{-1}X^{\top }  y
$$

- $$
  \hat{\theta}是使成本函数最小的\theta值
  $$

- $$
  y是样本中的含y^{1}到y^{m}的目标值向量
  $$

- $$
  X=\begin{pmatrix}1
    &x_{11}  &x_{12}  & .. &x_{1n}\\1
    &x_{21}  &x_{22}  &..  &2_{2n} \\: 
    &:  &:  &..  &: \\1
    &x_{m1}  &x_{m2}  &..  &x_{mn}
  \end{pmatrix}
  =\begin{pmatrix}1
    &x_{1}^{\top} \\1
    & x_{2}^{\top}\\1
    & :\\1
    &x_{m}^{\top}
  \end{pmatrix}
  其中x_{i}为第i个样本的输入
  $$

  

<u>代码验证：</u>

```python
#随机生成线性数据集
import numpy as np
#输入属性只有一个
X=2*np.random.rand(100,1) #创建一个100行1列的数组，用正在0，1上的均匀分布填充，并乘2
y=4+3*X+np.random.randn(100,1)  #y是x的线性函数加上误差

#绘制数据集图像
from matplotlib import pyplot as plt 
plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()
```

![image-20220726140720804](D:\home\markdown-notebook\assets\image-20220726140720804.png)

```python
#通过标准方程直接计算theta_hat
X_b=np.c_[np.ones((100,1)),X]   #先按定义拼错X矩阵
theta_best=np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)	#按公式计算
theta_best
#其中第一个为偏移，第二个为系数。
array([[4.08715471],
       [2.87714327]])
```

```python
#谁用求出的thtea_best做预测
X_new=np.array([[0],[2]])					#创建2个输入，一个二维列向量
X_new_b=np.c_[np.ones((2,1)),X_new]			#拼凑X矩阵
y_predict=X_new_b.dot(theta_best)			#由预测函数定义计算
y_predict									#输出结果
array([[4.08715471],
       [9.84144125]])
#绘制模型的预测结果(一个输入属性)
plt.plot(X_new,y_predict,'r-')
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```

![image-20220726141855405](D:\home\markdown-notebook\assets\image-20220726141855405.png)

<u>使用Scikit-Learn执行线性回归</u>

```python
from sklearn.linear_model import LinearRegression

#LinearRegression采用类似标准方程的方法计算模型参数，但不必考虑不可逆的情况，且更加高效
lin_reg=LinearRegression()
lin_reg.fit(X,y)						#X矩阵的格式同上
lin_reg.intercept_,lin_reg.coef_		#intercept_为项偏差，coef_为特征权重
(array([4.08715471]), array([[2.87714327]]))

```

## 计算复杂度

Scikit-Learn的LinearRegression类计算的复杂度约为 O（n^2）。如果你将特征数量加倍，那计算时间大约是原来的4倍。标准方程的复杂度为O(n^(2.4))到O(n^3)之间。

特征数量比较大（例如100 000）时，标准方程和SVD的计算将极其缓慢。好的一面是，相对于训练集中的实例数量（O（m））来 说，两个都是线性的，所以能够有效地处理大量的训练集，只要内存足 够。同样，线性回归模型一经训练（不论是标准方程还是其他算法），预测就非常快速：因为计算复杂度相对于想要预测的实例数量和特征数 
量来说都是线性的。

